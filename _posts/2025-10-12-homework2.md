---
layout: post
title: "Homework 2 - Analysis of Security Incident Distributions"
---

## Introduction

In this homework, I analyze a dataset of security incidents focusing on three key metrics:
- Incident Duration (minutes)
- Data Leaked (MB)
- Detection Time (hours)

These metrics are crucial for understanding patterns in security breaches and improving response strategies.

<!--more-->

## Theoretical Background

### Understanding Datasets

A **dataset** is a structured collection of data organized for analysis. In statistics and data science, datasets typically consist of:
- **Observations (rows)**: Individual data points or records
- **Variables (columns)**: Attributes or characteristics measured for each observation
- **Data types**: Different kinds of values (numerical, categorical, temporal, etc.)

Datasets serve as the foundation for statistical analysis, enabling us to identify patterns, test hypotheses, and make data-driven decisions. In the context of cybersecurity, datasets help organizations understand incident patterns and improve their security posture.

### Statistical Distributions

A **statistical distribution** describes how values of a variable are spread or distributed across different possible outcomes. Understanding distributions is essential because:

1. **Pattern Recognition**: Distributions reveal the underlying structure of data
2. **Prediction**: They help forecast future events based on historical patterns
3. **Comparison**: Distributions allow us to compare different datasets or time periods
4. **Decision Making**: They provide a basis for risk assessment and resource allocation

#### Key Distribution Types

Different phenomena follow different distribution patterns:

- **Normal (Gaussian) Distribution**: Symmetrical, bell-shaped curve where data clusters around a central mean. Common in natural phenomena and processes with multiple random factors.

- **Log-normal Distribution**: Right-skewed distribution where the logarithm of the variable is normally distributed. Often appears in scenarios involving multiplicative processes, such as file sizes or monetary values.

- **Exponential Distribution**: Models the time between events in a Poisson process. Commonly used for waiting times, lifetimes, and detection/response scenarios.

Each distribution has specific parameters that define its shape and characteristics, making it suitable for modeling different real-world scenarios.

## Methodology

### Database Setup

To store and manage the security incident data, I created a PostgreSQL database table with appropriate constraints to ensure data integrity:

```sql
CREATE TABLE IF NOT EXISTS public.security_incidents (
    id SERIAL PRIMARY KEY,
    incident_duration INTEGER CHECK (incident_duration > 0),
    data_leaked DECIMAL CHECK (data_leaked >= 0),
    detection_time DECIMAL CHECK (detection_time >= 0)
);
```

This schema includes:
- An auto-incrementing primary key (`id`) for unique identification
- `incident_duration`: Positive integer values representing minutes
- `data_leaked`: Non-negative decimal values representing megabytes
- `detection_time`: Non-negative decimal values representing hours
- CHECK constraints to ensure data validity

#### Initial Testing

Before generating the full dataset, I performed a test insertion to verify the table structure:

```sql
INSERT INTO public.security_incidents (incident_duration, data_leaked, detection_time)
VALUES (45, 2.5, 1.5);
```

This test record confirmed that the table was correctly configured and ready to receive the generated data.

### Dataset Composition

The final dataset consists of **31 security incidents**:
- 1 test record (manually inserted for validation)
- 30 records generated through the Python script

Below is a representative sample of the data stored in the database:

| id | incident_duration | data_leaked | detection_time |
|----|-------------------|-------------|----------------|
| 1  | 45                | 2.50        | 1.50           |
| 2  | 64                | 1.69        | 0.95           |
| 3  | 70                | 0.29        | 0.67           |
| 4  | 55                | 20.08        | 5.38           |
| 5  | 46                | 1.93        | 0.19           |
| ... | ...              | ...         | ...            |
| 31 | 75                | 5.09        | 4.71           |

*Note: This table shows a representative sample of the dataset. The complete dataset with all 31 records is available in the project repository as [security_incidents_table.csv](/assets/data/homework2/security_incidents_table.csv).*

### Data Generation and Storage

To create a realistic dataset, I implemented two Python scripts for data generation and analysis:

<details>
    <summary>View generate_security_data.py</summary>
    {% highlight python linenos %}
    import numpy as np
    import psycopg2
    from datetime import datetime
    from dotenv import load_dotenv
    import os

    load_dotenv()

    db_params = {
        "database": os.getenv('DB_NAME'),
        "user": os.getenv('DB_USER'),
        "password": os.getenv('DB_PASSWORD'),
        "host": os.getenv('DB_HOST'),
        "port": os.getenv('DB_PORT')
    }

    def generate_security_incidents(n_samples=30):
        # Generate incident durations (convert to standard Python int)
        incident_durations = [int(x) for x in np.random.normal(60, 20, n_samples)]
        incident_durations = [max(1, x) for x in incident_durations]
        
        # Generate data leaked (convert to float)
        data_leaked = [float(round(x, 2)) for x in np.random.lognormal(0, 1, n_samples) * 2]
        
        # Generate detection times (convert to float)
        detection_times = [float(round(x, 2)) for x in np.random.exponential(2, n_samples)]
        
        return list(zip(incident_durations, data_leaked, detection_times))

    try:
        conn = psycopg2.connect(**db_params)
        cur = conn.cursor()
        incidents = generate_security_incidents()
        
        for incident in incidents:
            cur.execute("""
                INSERT INTO security_incidents 
                (incident_duration, data_leaked, detection_time)
                VALUES (%s, %s, %s)
            """, incident)
        
        conn.commit()
        print(f"Successfully inserted {len(incidents)} records")

    except Exception as e:
        print(f"Error: {e}")
    finally:
        if conn:
            cur.close()
            conn.close()
    {% endhighlight %}
</details>

<details>
    <summary>View analyze_distributions.py</summary>
    {% highlight python linenos %}
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import psycopg2
    import pandas as pd
    from sqlalchemy import create_engine
    from dotenv import load_dotenv
    import os

    load_dotenv()

    db_params = {
        "database": os.getenv('DB_NAME'),
        "user": os.getenv('DB_USER'),
        "password": os.getenv('DB_PASSWORD'),
        "host": os.getenv('DB_HOST'),
        "port": os.getenv('DB_PORT')
    }

    def fetch_data():
        engine = create_engine(f'postgresql://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["database"]}')
        query = "SELECT incident_duration, data_leaked, detection_time FROM security_incidents"
        df = pd.read_sql_query(query, engine)
        return df

    def plot_distributions(df):
        os.makedirs('assets/images', exist_ok=True)

        # Plot Incident Duration
        plt.figure(figsize=(10, 6))
        sns.histplot(data=df, x='incident_duration', kde=True, color='skyblue')
        plt.title('Distribution of Incident Duration')
        plt.xlabel('Duration (minutes)')
        plt.ylabel('Frequency')
        plt.savefig('assets/images/incident_duration.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot Data Leaked
        plt.figure(figsize=(10, 6))
        sns.histplot(data=df, x='data_leaked', kde=True, color='lightgreen')
        plt.title('Distribution of Data Leaked')
        plt.xlabel('Data Leaked (MB)')
        plt.ylabel('Frequency')
        plt.savefig('assets/images/data_leaked.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot Detection Time
        plt.figure(figsize=(10, 6))
        sns.histplot(data=df, x='detection_time', kde=True, color='salmon')
        plt.title('Distribution of Detection Time')
        plt.xlabel('Detection Time (hours)')
        plt.ylabel('Frequency')
        plt.savefig('assets/images/detection_time.png', dpi=300, bbox_inches='tight')
        plt.close()

    def main():
        df = fetch_data()
        plot_distributions(df)
        print("\nBasic Statistics:")
        print(df.describe())

    if __name__ == "__main__":
        main()
    {% endhighlight %}
</details>

The implementation follows this approach:

1. **Database Setup**: Created a PostgreSQL table `security_incidents` with columns for incident duration, data leaked, and detection time
2. **Data Generation**: Used NumPy to generate statistically realistic data:
   - Normal distribution for incident durations (mean = 60 minutes, std = 20)
   - Log-normal distribution for data leaks (scaled by factor of 2)
   - Exponential distribution for detection times (rate parameter = 2)
3. **Data Storage**: Utilized psycopg2 to efficiently store the generated data
4. **Analysis**: Generated distribution plots using Matplotlib and Seaborn, with kernel density estimation (KDE) curves

### Visualization

The distributions were visualized using Python's Matplotlib and Seaborn libraries, creating histograms with kernel density estimation (KDE) curves to better understand the data patterns.

## Dataset Description and Analysis

The dataset consists of 31 security incidents, each following specific statistical distributions:

### 1. Incident Duration (Normal Distribution)

![Incident Duration Distribution](/assets/images/homework2/incident_duration.png)

The incident duration data shows:
- Mean duration: 64.32 minutes
- Standard deviation: 15.29 minutes
- Range: 37 to 99 minutes
- Median (50th percentile): 64 minutes

The distribution appears approximately normal, clustering around the one-hour mark, which aligns with typical security incident patterns.

### 2. Data Leaked (Log-normal Distribution)

![Data Leaked Distribution](/assets/images/homework2/data_leaked.png)

The data leaked measurements reveal:
- Mean: 3.81 MB
- Standard deviation: 4.53 MB
- Range: 0.22 to 20.08 MB
- Median: 2.34 MB

The right-skewed distribution indicates that while most breaches involve smaller amounts of data, there are occasional large-scale data leaks, which is typical in real-world scenarios.

### 3. Detection Time (Exponential Distribution)

![Detection Time Distribution](/assets/images/homework2/detection_time.png)

Detection time statistics show:
- Mean: 1.26 hours
- Standard deviation: 1.81 hours
- Range: 0.10 to 8.38 hours
- Median: 0.67 hours

The exponential pattern indicates that most incidents are detected relatively quickly, but some take significantly longer, which is consistent with real-world cybersecurity scenarios.

## Statistical Insights

The quartile analysis reveals important operational insights:
- 75% of incidents are resolved within 75.5 minutes
- 75% of data breaches involve less than 4.7 MB of data
- 75% of incidents are detected within 1.07 hours

These metrics provide valuable benchmarks for:
- Setting response time objectives
- Allocating security resources
- Developing incident response strategies

## Conclusion

The statistical analysis reveals patterns that can inform cybersecurity strategies:

1. **Incident durations** follow a predictable normal pattern, useful for resource planning and staffing decisions
2. **Data leaks** show a long-tail distribution, highlighting the need for scalable response capabilities to handle both typical and exceptional cases
3. **Detection times** demonstrate the importance of early warning systems, as most incidents benefit from rapid detection

These insights can help organizations better prepare for and respond to security incidents, particularly in allocating resources and setting response time targets. Understanding these distributions allows security teams to establish realistic benchmarks and identify anomalies that may require special attention.
