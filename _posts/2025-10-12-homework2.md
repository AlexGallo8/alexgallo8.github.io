---
layout: post
title: "Homework 2 - Explain the concept of Dataset and Distribution"
---
## Part 1: Analysis of Security Incident Distributions

## Introduction to Statistical Analysis

In this homework, I analyze a dataset of security incidents focusing on three key metrics:
- Incident Duration (minutes)
- Data Leaked (MB)
- Detection Time (hours)

These metrics are crucial for understanding patterns in security breaches and improving response strategies.

<!--more-->

## Theoretical Background

### Understanding Datasets

A **dataset** is a structured collection of data organized for analysis. In statistics and data science, datasets typically consist of:
- **Observations (rows)**: Individual data points or records
- **Variables (columns)**: Attributes or characteristics measured for each observation
- **Data types**: Different kinds of values (numerical, categorical, temporal, etc.)

Datasets serve as the foundation for statistical analysis, enabling us to identify patterns, test hypotheses, and make data-driven decisions. In the context of cybersecurity, datasets help organizations understand incident patterns and improve their security posture.

### Statistical Distributions

A **statistical distribution** describes how values of a variable are spread or distributed across different possible outcomes. Understanding distributions is essential because:

1. **Pattern Recognition**: Distributions reveal the underlying structure of data
2. **Prediction**: They help forecast future events based on historical patterns
3. **Comparison**: Distributions allow us to compare different datasets or time periods
4. **Decision Making**: They provide a basis for risk assessment and resource allocation

#### Key Distribution Types

Different phenomena follow different distribution patterns:

- **Normal (Gaussian) Distribution**: Symmetrical, bell-shaped curve where data clusters around a central mean. Common in natural phenomena and processes with multiple random factors.

- **Log-normal Distribution**: Right-skewed distribution where the logarithm of the variable is normally distributed. Often appears in scenarios involving multiplicative processes, such as file sizes or monetary values.

- **Exponential Distribution**: Models the time between events in a Poisson process. Commonly used for waiting times, lifetimes, and detection/response scenarios.

Each distribution has specific parameters that define its shape and characteristics, making it suitable for modeling different real-world scenarios.

## Methodology

### Database Setup

To store and manage the security incident data, I created a PostgreSQL database table with appropriate constraints to ensure data integrity:

```sql
CREATE TABLE IF NOT EXISTS public.security_incidents (
    id SERIAL PRIMARY KEY,
    incident_duration INTEGER CHECK (incident_duration > 0),
    data_leaked DECIMAL CHECK (data_leaked >= 0),
    detection_time DECIMAL CHECK (detection_time >= 0)
);
```

This schema includes:
- An auto-incrementing primary key (`id`) for unique identification
- `incident_duration`: Positive integer values representing minutes
- `data_leaked`: Non-negative decimal values representing megabytes
- `detection_time`: Non-negative decimal values representing hours
- CHECK constraints to ensure data validity

#### Initial Testing

Before generating the full dataset, I performed a test insertion to verify the table structure:

```sql
INSERT INTO public.security_incidents (incident_duration, data_leaked, detection_time)
VALUES (45, 2.5, 1.5);
```

This test record confirmed that the table was correctly configured and ready to receive the generated data.

### Dataset Composition

The final dataset consists of **31 security incidents**:
- 1 test record (manually inserted for validation)
- 30 records generated through the Python script

Below is a representative sample of the data stored in the database:

| id | incident_duration | data_leaked | detection_time |
|----|-------------------|-------------|----------------|
| 1  | 45                | 2.50        | 1.50           |
| 2  | 67                | 1.23        | 0.85           |
| 3  | 52                | 5.67        | 2.34           |
| 4  | 81                | 0.92        | 0.45           |
| 5  | 59                | 3.14        | 1.78           |
| 6  | 72                | 4.89        | 2.15           |
| 7  | 38                | 1.56        | 0.33           |
| 8  | 91                | 8.21        | 3.67           |
| ... | ...              | ...         | ...            |
| 31 | 64                | 2.34        | 0.67           |

*Note: This table shows a representative sample of the dataset. The complete dataset with all 31 records is available in the project repository as [security_incidents.csv](/assets/data/homework2/security_incidents.csv).*

### Data Generation and Storage

To create a realistic dataset, I implemented two Python scripts for data generation and analysis:

<details>
    <summary>View generate_security_data.py</summary>
    {% highlight python linenos %}
    import numpy as np
    import psycopg2
    from datetime import datetime
    from dotenv import load_dotenv
    import os

    load_dotenv()

    db_params = {
        "database": os.getenv('DB_NAME'),
        "user": os.getenv('DB_USER'),
        "password": os.getenv('DB_PASSWORD'),
        "host": os.getenv('DB_HOST'),
        "port": os.getenv('DB_PORT')
    }

    def generate_security_incidents(n_samples=30):
        # Generate incident durations (convert to standard Python int)
        incident_durations = [int(x) for x in np.random.normal(60, 20, n_samples)]
        incident_durations = [max(1, x) for x in incident_durations]
        
        # Generate data leaked (convert to float)
        data_leaked = [float(round(x, 2)) for x in np.random.lognormal(0, 1, n_samples) * 2]
        
        # Generate detection times (convert to float)
        detection_times = [float(round(x, 2)) for x in np.random.exponential(2, n_samples)]
        
        return list(zip(incident_durations, data_leaked, detection_times))

    try:
        conn = psycopg2.connect(**db_params)
        cur = conn.cursor()
        incidents = generate_security_incidents()
        
        for incident in incidents:
            cur.execute("""
                INSERT INTO security_incidents 
                (incident_duration, data_leaked, detection_time)
                VALUES (%s, %s, %s)
            """, incident)
        
        conn.commit()
        print(f"Successfully inserted {len(incidents)} records")

    except Exception as e:
        print(f"Error: {e}")
    finally:
        if conn:
            cur.close()
            conn.close()
    {% endhighlight %}
</details>

<details>
    <summary>View analyze_distributions.py</summary>
    {% highlight python linenos %}
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import psycopg2
    import pandas as pd
    from sqlalchemy import create_engine
    from dotenv import load_dotenv
    import os

    load_dotenv()

    db_params = {
        "database": os.getenv('DB_NAME'),
        "user": os.getenv('DB_USER'),
        "password": os.getenv('DB_PASSWORD'),
        "host": os.getenv('DB_HOST'),
        "port": os.getenv('DB_PORT')
    }

    def fetch_data():
        engine = create_engine(f'postgresql://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["database"]}')
        query = "SELECT incident_duration, data_leaked, detection_time FROM security_incidents"
        df = pd.read_sql_query(query, engine)
        return df

    def plot_distributions(df):
        os.makedirs('assets/images', exist_ok=True)

        # Plot Incident Duration
        plt.figure(figsize=(10, 6))
        sns.histplot(data=df, x='incident_duration', kde=True, color='skyblue')
        plt.title('Distribution of Incident Duration')
        plt.xlabel('Duration (minutes)')
        plt.ylabel('Frequency')
        plt.savefig('assets/images/incident_duration.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot Data Leaked
        plt.figure(figsize=(10, 6))
        sns.histplot(data=df, x='data_leaked', kde=True, color='lightgreen')
        plt.title('Distribution of Data Leaked')
        plt.xlabel('Data Leaked (MB)')
        plt.ylabel('Frequency')
        plt.savefig('assets/images/data_leaked.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot Detection Time
        plt.figure(figsize=(10, 6))
        sns.histplot(data=df, x='detection_time', kde=True, color='salmon')
        plt.title('Distribution of Detection Time')
        plt.xlabel('Detection Time (hours)')
        plt.ylabel('Frequency')
        plt.savefig('assets/images/detection_time.png', dpi=300, bbox_inches='tight')
        plt.close()

    def main():
        df = fetch_data()
        plot_distributions(df)
        print("\nBasic Statistics:")
        print(df.describe())

    if __name__ == "__main__":
        main()
    {% endhighlight %}
</details>

The implementation follows this approach:

1. **Database Setup**: Created a PostgreSQL table `security_incidents` with columns for incident duration, data leaked, and detection time
2. **Data Generation**: Used NumPy to generate statistically realistic data:
   - Normal distribution for incident durations (mean = 60 minutes, std = 20)
   - Log-normal distribution for data leaks (scaled by factor of 2)
   - Exponential distribution for detection times (rate parameter = 2)
3. **Data Storage**: Utilized psycopg2 to efficiently store the generated data
4. **Analysis**: Generated distribution plots using Matplotlib and Seaborn, with kernel density estimation (KDE) curves

### Visualization

The distributions were visualized using Python's Matplotlib and Seaborn libraries, creating histograms with kernel density estimation (KDE) curves to better understand the data patterns.

## Dataset Description and Analysis

The dataset consists of 31 security incidents, each following specific statistical distributions:

### 1. Incident Duration (Normal Distribution)

![Incident Duration Distribution](/assets/images/homework2/incident_duration.png)

The incident duration data shows:
- Mean duration: 64.32 minutes
- Standard deviation: 15.29 minutes
- Range: 37 to 99 minutes
- Median (50th percentile): 64 minutes

The distribution appears approximately normal, clustering around the one-hour mark, which aligns with typical security incident patterns.

### 2. Data Leaked (Log-normal Distribution)

![Data Leaked Distribution](/assets/images/homework2/data_leaked.png)

The data leaked measurements reveal:
- Mean: 3.81 MB
- Standard deviation: 4.53 MB
- Range: 0.22 to 20.08 MB
- Median: 2.34 MB

The right-skewed distribution indicates that while most breaches involve smaller amounts of data, there are occasional large-scale data leaks, which is typical in real-world scenarios.

### 3. Detection Time (Exponential Distribution)

![Detection Time Distribution](/assets/images/homework2/detection_time.png)

Detection time statistics show:
- Mean: 1.26 hours
- Standard deviation: 1.81 hours
- Range: 0.10 to 8.38 hours
- Median: 0.67 hours

The exponential pattern indicates that most incidents are detected relatively quickly, but some take significantly longer, which is consistent with real-world cybersecurity scenarios.

## Statistical Insights

The quartile analysis reveals important operational insights:
- 75% of incidents are resolved within 75.5 minutes
- 75% of data breaches involve less than 4.7 MB of data
- 75% of incidents are detected within 1.07 hours

These metrics provide valuable benchmarks for:
- Setting response time objectives
- Allocating security resources
- Developing incident response strategies

## Conclusion

The statistical analysis reveals patterns that can inform cybersecurity strategies:

1. **Incident durations** follow a predictable normal pattern, useful for resource planning and staffing decisions
2. **Data leaks** show a long-tail distribution, highlighting the need for scalable response capabilities to handle both typical and exceptional cases
3. **Detection times** demonstrate the importance of early warning systems, as most incidents benefit from rapid detection

These insights can help organizations better prepare for and respond to security incidents, particularly in allocating resources and setting response time targets. Understanding these distributions allows security teams to establish realistic benchmarks and identify anomalies that may require special attention.

---

## Part 2: Caesar Cipher and Frequency Analysis

## Introduction to Cryptographic Analysis

In this second part of the homework, I explore the Caesar cipher, one of the oldest and simplest encryption techniques, and demonstrate how statistical frequency analysis can be used to decrypt messages without knowing the encryption key. This exercise illustrates the fundamental relationship between statistical distributions and cryptographic security.

## Theoretical Background

### The Caesar Cipher

The **Caesar cipher** is a substitution cipher where each letter in the plaintext is shifted a fixed number of positions in the alphabet. Named after Julius Caesar, who reportedly used it for military communications, it represents one of the earliest known encryption methods.

**Mathematical representation:**
- Encryption: `E(x) = (x + k) mod 26`
- Decryption: `D(x) = (x - k) mod 26`

Where `x` is the letter position (0-25) and `k` is the shift value (encryption key).

### Frequency Analysis Attack

Frequency analysis exploits the fact that letters in natural language occur with predictable frequencies. By comparing the letter distribution in encrypted text with the expected distribution of the target language, we can identify the most likely shift value.

**Key principle:** The statistical properties of language are preserved under simple substitution ciphers, making them vulnerable to frequency-based attacks.

## Methodology

### Dataset Selection

For this analysis, I selected the first chapter of [**"The Hound of the Baskervilles"**](https://www.gutenberg.org/files/2852/2852-h/2852-h.htm#chap01) by Sir Arthur Conan Doyle (1902), a public domain work available through Project Gutenberg. The text contains more than 5,400 characters, providing a sufficiently large sample for reliable statistical analysis.

The choice of a literary text ensures:
- Rich vocabulary and natural language patterns
- Sufficient length for accurate frequency distribution
- Representative English language characteristics

### Implementation Workflow

The complete analysis follows these steps:

1. **Text Input**: Load the original plaintext from file
2. **Random Encryption**: Apply Caesar cipher with random shift (1-25)
3. **Frequency Analysis**: Calculate letter frequencies in both original and encrypted texts
4. **Automated Decryption**: Use chi-squared test to identify the correct shift
5. **Visualization**: Generate comparative plots of frequency distributions
6. **Validation**: Verify successful decryption

### Python Implementation

<details>
    <summary>View caesar_cipher.py</summary>
    {% highlight python linenos %}
import string
import random
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import Counter
import os

# Expected letter frequencies for different languages (in percentages)
LANGUAGE_FREQUENCIES = {
    # Source: Peter Norvig's analysis of Google Books corpus (2013)
    # http://norvig.com/mayzner.html
    # Based on analysis of 3.5 trillion characters from Google Books
    'english': {
        'e': 12.49, 't': 9.28, 'a': 8.04, 'o': 7.64, 'i': 7.57, 'n': 7.23,
        's': 6.51, 'r': 6.28, 'h': 5.05, 'l': 4.07, 'd': 3.82, 'c': 3.34,
        'u': 2.73, 'm': 2.51, 'f': 2.40, 'p': 2.14, 'g': 1.87, 'w': 1.68,
        'y': 1.66, 'b': 1.48, 'v': 1.05, 'k': 0.54, 'x': 0.23, 'j': 0.16,
        'q': 0.12, 'z': 0.09
    },
    # Source: Wikipedia - Letter Frequency
    # https://en.wikipedia.org/wiki/Letter_frequency
    # Based on analysis of Italian text corpora
    'italian': {
        'e': 11.79, 'a': 11.74, 'i': 11.28, 'o': 9.83, 'n': 6.88, 'l': 6.51,
        'r': 6.37, 't': 5.62, 's': 4.98, 'c': 4.50, 'd': 3.73, 'p': 3.05,
        'u': 3.01, 'm': 2.51, 'v': 2.10, 'g': 1.64, 'h': 1.54, 'f': 0.95,
        'b': 0.92, 'q': 0.51, 'z': 0.49, 'j': 0.00, 'k': 0.00, 'w': 0.00,
        'x': 0.00, 'y': 0.00
    }
}

def caesar_encrypt(text, shift):
    """
    Encrypts text using Caesar cipher with given shift.
    Only encrypts letters, preserves case and other characters.
    """
    result = []
    for char in text:
        if char.isalpha():
            base = ord('A') if char.isupper() else ord('a')
            shifted = (ord(char) - base + shift) % 26
            result.append(chr(base + shifted))
        else:
            result.append(char)
    return ''.join(result)

def analyze_frequency(text):
    """
    Analyzes letter frequency in text.
    Returns dictionary of letter: percentage and total letter count.
    """
    letters = [c.lower() for c in text if c.isalpha()]
    total = len(letters)
    
    if total == 0:
        return {}, 0
    
    counts = Counter(letters)
    frequencies = {letter: (count / total) * 100 
                   for letter, count in counts.items()}
    
    for letter in string.ascii_lowercase:
        if letter not in frequencies:
            frequencies[letter] = 0.0
    
    return dict(sorted(frequencies.items())), total

def chi_squared_score(observed_freq, expected_freq):
    """
    Calculates chi-squared statistic between observed and expected frequencies.
    Lower score = better match.
    """
    score = 0
    for letter in string.ascii_lowercase:
        expected = expected_freq.get(letter, 0.1)
        observed = observed_freq.get(letter, 0)
        score += ((observed - expected) ** 2) / expected
    return score

def decrypt_with_frequency_analysis(encrypted_text, language='english'):
    """
    Decrypts Caesar cipher using frequency analysis.
    Tries all 26 possible shifts and returns the one with best match to language.
    """
    expected_freq = LANGUAGE_FREQUENCIES[language]
    best_shift = 0
    best_score = float('inf')
    results = []
    
    for shift in range(26):
        decrypted = caesar_encrypt(encrypted_text, shift)
        observed_freq, _ = analyze_frequency(decrypted)
        score = chi_squared_score(observed_freq, expected_freq)
        
        results.append({
            'shift': shift,
            'score': score,
            'decrypted_preview': decrypted[:100]
        })
        
        if score < best_score:
            best_score = score
            best_shift = shift
    
    best_decryption = caesar_encrypt(encrypted_text, best_shift)
    
    return {
        'best_shift': best_shift,
        'original_shift': (26 - best_shift) % 26,
        'decrypted_text': best_decryption,
        'chi_squared_score': best_score,
        'all_attempts': sorted(results, key=lambda x: x['score'])
    }

def plot_frequency_comparison(original_freq, encrypted_freq, decrypted_freq, 
                              expected_freq, language, output_dir='assets/images'):
    """Creates comparison plots of letter frequencies."""
    os.makedirs(output_dir, exist_ok=True)
    letters = list(string.ascii_lowercase)
    
    orig_values = [original_freq.get(l, 0) for l in letters]
    enc_values = [encrypted_freq.get(l, 0) for l in letters]
    dec_values = [decrypted_freq.get(l, 0) for l in letters]
    exp_values = [expected_freq.get(l, 0) for l in letters]
    
    # Plot 1: Original vs Expected
    fig, ax = plt.subplots(figsize=(14, 6))
    x = range(len(letters))
    width = 0.35
    ax.bar([i - width/2 for i in x], orig_values, width, label='Original Text', alpha=0.8)
    ax.bar([i + width/2 for i in x], exp_values, width, label=f'Expected ({language.title()})', alpha=0.8)
    ax.set_xlabel('Letters')
    ax.set_ylabel('Frequency (%)')
    ax.set_title('Original Text vs Expected Language Distribution')
    ax.set_xticks(x)
    ax.set_xticklabels(letters)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/original_vs_expected.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Plot 2: Encrypted frequency
    fig, ax = plt.subplots(figsize=(14, 6))
    ax.bar(letters, enc_values, color='coral', alpha=0.8)
    ax.set_xlabel('Letters')
    ax.set_ylabel('Frequency (%)')
    ax.set_title('Encrypted Text Letter Distribution')
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/encrypted_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Plot 3: Encrypted vs Expected (for visual shift detection)
    fig, ax = plt.subplots(figsize=(14, 6))
    width = 0.35
    ax.bar([i - width/2 for i in x], enc_values, width, label='Encrypted Text', alpha=0.8, color='coral')
    ax.bar([i + width/2 for i in x], exp_values, width, label=f'Expected ({language.title()})', alpha=0.8, color='steelblue')
    ax.set_xlabel('Letters')
    ax.set_ylabel('Frequency (%)')
    ax.set_title('Encrypted vs Expected Distribution - Visual Shift Detection')
    ax.set_xticks(x)
    ax.set_xticklabels(letters)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/encrypted_vs_expected.png', dpi=300, bbox_inches='tight')
    plt.close()

    # Plot 4: All distributions comparison
    fig, ax = plt.subplots(figsize=(16, 8))
    width = 0.2
    x = range(len(letters))
    ax.bar([i - 1.5*width for i in x], orig_values, width, label='Original', alpha=0.8)
    ax.bar([i - 0.5*width for i in x], enc_values, width, label='Encrypted', alpha=0.8)
    ax.bar([i + 0.5*width for i in x], dec_values, width, label='Decrypted', alpha=0.8)
    ax.bar([i + 1.5*width for i in x], exp_values, width, label=f'Expected ({language.title()})', alpha=0.8)
    ax.set_xlabel('Letters')
    ax.set_ylabel('Frequency (%)')
    ax.set_title('Complete Frequency Analysis Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(letters)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/complete_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def save_frequency_table(frequencies, filename):
    """Saves frequency data to CSV file."""
    df = pd.DataFrame([
        {'letter': letter, 'frequency_%': freq}
        for letter, freq in sorted(frequencies.items())
    ])
    df.to_csv(filename, index=False)

def main():
    print("=== Caesar Cipher Frequency Analysis ===\n")
    
    with open('assets/files/input_text.txt', 'r', encoding='utf-8') as f:
        original_text = f.read()
    
    language = 'english'
    
    original_shift = random.randint(1, 25)
    print(f"Random shift applied: {original_shift}")
    encrypted_text = caesar_encrypt(original_text, original_shift)
    
    with open('assets/files/encrypted_text.txt', 'w', encoding='utf-8') as f:
        f.write(encrypted_text)
    
    original_freq, orig_count = analyze_frequency(original_text)
    encrypted_freq, enc_count = analyze_frequency(encrypted_text)
    
    os.makedirs('assets/data/homework2', exist_ok=True)
    save_frequency_table(original_freq, 'assets/data/homework2/original_frequencies.csv')
    save_frequency_table(encrypted_freq, 'assets/data/homework2/encrypted_frequencies.csv')
    
    result = decrypt_with_frequency_analysis(encrypted_text, language)
    
    print(f"\n=== RESULTS ===")
    print(f"Original shift: {original_shift}")
    print(f"Detected shift: {result['original_shift']}")
    print(f"Success: {'YES' if result['original_shift'] == original_shift else 'NO'}")
    
    decrypted_freq, _ = analyze_frequency(result['decrypted_text'])
    save_frequency_table(decrypted_freq, 'assets/data/homework2/decrypted_frequencies.csv')
    
    with open('assets/files/decrypted_text.txt', 'w', encoding='utf-8') as f:
        f.write(result['decrypted_text'])
    
    expected_freq = LANGUAGE_FREQUENCIES[language]
    plot_frequency_comparison(original_freq, encrypted_freq, decrypted_freq, 
                             expected_freq, language, 'assets/images/homework2')

if __name__ == "__main__":
    main()
    {% endhighlight %}
</details>

### Reference Letter Frequencies

The expected letter frequencies are based on large-scale corpus analysis:
- **English frequencies**: Peter Norvig's analysis of 3.5 trillion characters from Google Books (2013)
- **Italian frequencies**: Compiled from multiple linguistic corpora studies

These frequencies serve as the reference distribution for the chi-squared goodness-of-fit test.

## Results and Analysis

### Original Text Characteristics

The selected text excerpt contains 5,423 alphabetic characters. The letter frequency distribution closely matches the expected English language pattern:

![Original vs Expected Distribution](/assets/images/homework2/original_vs_expected.png)

*Figure 1: Comparison between the original text distribution and expected English letter frequencies*

The high correlation confirms that the text is representative of standard English prose, making it suitable for demonstrating frequency analysis techniques.

### Encryption Process

A random shift value was applied to encrypt the text. The encryption preserves:
- Non-alphabetic characters (spaces, punctuation)
- Letter case (uppercase/lowercase)
- Text structure and length

Only the alphabetic characters are transformed through the Caesar cipher substitution.

### Encrypted Text Analysis

![Encrypted Distribution](/assets/images/homework2/encrypted_distribution.png)

*Figure 2: Letter frequency distribution of the encrypted text*

The encrypted text maintains a similar distribution shape to the original, but shifted horizontally across the alphabet. This preservation of statistical patterns is the fundamental weakness of simple substitution ciphers.

### Visual Shift Detection Algorithm

![Encrypted vs Expected Comparison](/assets/images/homework2/encrypted_vs_expected.png)

*Figure 3: Side-by-side comparison for visual shift identification*

**Manual decryption algorithm:**
1. Identify the highest frequency letter in the encrypted text
2. Compare its position with 'e' (most common in English)
3. Calculate the horizontal distance between peaks
4. This distance represents the encryption shift value
5. Verify by checking secondary peaks ('t', 'a', 'o')

This visual method allows cryptanalysts to estimate the shift without computational tools.

### Automated Decryption: Chi-Squared Test

The automated decryption algorithm tests all 26 possible shifts and evaluates each using the chi-squared statistic:

**Chi-squared formula:**
```
χ² = Σ [(observed - expected)² / expected]
```

For each potential shift:
- Decrypt the text
- Calculate letter frequencies
- Compute χ² score against expected English distribution
- Lower score indicates better match

**Results:**
- **Original encryption shift**: 18
- **Detected decryption shift**: 8
- **Computed original shift**: (26 - 8) mod 26 = 18 ✓
- **Chi-squared score**: 1.69 (excellent match)
- **Success rate**: 100%

The relationship between encryption and decryption shifts:
```
encryption_shift + decryption_shift = 26 (mod 26)
```

### Complete Frequency Comparison

![Complete Distribution Comparison](/assets/images/homework2/complete_comparison.png)

*Figure 4: Comprehensive comparison of all frequency distributions*

This visualization demonstrates:
- **Original** and **Decrypted** distributions are identical (successful decryption)
- **Encrypted** distribution is a horizontal shift of the original
- Both distributions closely follow the **Expected** English pattern

### Frequency Tables

Detailed frequency data is available in CSV format:
- [Original text frequencies](/assets/data/homework2/original_frequencies.csv)
- [Encrypted text frequencies](/assets/data/homework2/encrypted_frequencies.csv)
- [Decrypted text frequencies](/assets/data/homework2/decrypted_frequencies.csv)

## Cryptographic Insights

### Why Caesar Cipher is Insecure

1. **Small keyspace**: Only 25 meaningful keys (shifts 1-25), vulnerable to brute force
2. **Frequency preservation**: Statistical patterns remain unchanged
3. **No diffusion**: Each plaintext letter maps to exactly one ciphertext letter
4. **Deterministic**: Same input always produces same output

### Statistical Attack Effectiveness

The frequency analysis attack succeeds because:
- Natural language has non-uniform letter distribution
- Large text samples converge to expected frequencies
- Chi-squared test provides objective comparison metric
- No knowledge of the key is required

**Minimum text length**: Effective frequency analysis typically requires 100-200 characters, though accuracy improves significantly with longer texts.

## Conclusion

This analysis demonstrates the fundamental vulnerability of simple substitution ciphers to statistical attacks. Key findings:

1. **Distribution preservation**: The Caesar cipher maintains the statistical properties of the plaintext, making it vulnerable to frequency analysis

2. **Automated decryption**: Using chi-squared testing, the correct shift can be identified with 100% accuracy for sufficiently long texts

3. **Visual detection**: Even without computational tools, the shift can be estimated by comparing frequency distributions visually

4. **Historical significance**: While cryptographically weak by modern standards, the Caesar cipher illustrates important concepts in both cryptography and statistical analysis

**Modern relevance**: This exercise demonstrates why modern encryption relies on:
- Large keyspaces (making brute force impractical)
- Diffusion (spreading patterns across ciphertext)
- Confusion (complex relationship between key and ciphertext)
- Non-deterministic encryption (same plaintext produces different ciphertext)

Understanding these classical attacks provides foundational knowledge for appreciating contemporary cryptographic systems.